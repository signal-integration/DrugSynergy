---
title: "X+Y analysis using random forests"
output: html_document
---

<br />
#### Simulating mean expression for a given integration profile
<br />
Here we generate numerical values (e_0, e_X, e_Y, e_X+Y) representing a given profile. 


```{r message = FALSE, warning = FALSE}
source("compute_profile_means.R")
source("setPowerPointStyle.R")
setPowerPointStyle()

#reading the profile definitions
PROFCODES = read.table("profile_codes_v2.txt",header = TRUE,sep = "\t") 

#this file encodes the system of equalities/inequalities that we want to solve
load("constraints_vector")

prof_index = 3 #index of the profile we want to simulate
ntimes = 5 #n. of simulations
exp_min = 2 #min range of expression value
exp_max = 16 #max range of expression value
min_delta = 1 #minimum non-zero difference among any two comparisons

profile_means = compute_profile_means(PROFCODES, prof_index, ntimes, 
                                      exp_min, exp_max, 
                                      constraints_vector, min_delta)[ , 1:4]

colnames(profile_means)=c("0","X","Y","X+Y")
head(profile_means)
```


#### Plotting one example
```{r message = FALSE, warning = FALSE}
source("setPowerPointStyle.R")
setPowerPointStyle()

barplot(profile_means[1,], ylab = 'simulated expression')
delta_x = profile_means[1,2] - profile_means[1,1]
delta_y = profile_means[1,3] - profile_means[1,1]
add = profile_means[1,1] + delta_x + delta_y
abline(h = add, col="red")
```


#### Simulating random samples for a given inegration profile
After computing the means for a given profile, we can generate random samples resembling real data. We assume that real data come from normal distributions centered around the means computed above. The standard deviation is passed as a parameter, so it is possible to simulate  arbitrary noise levels.

```{r results='hide', message=FALSE, warning=FALSE}
source("simulate_from_means.R")
source("setPowerPointStyle.R")
setPowerPointStyle()

samples = 4 #number of samples for each condition that will be simulated

noise_level = 0.5 #this means that the signal-to-noise is delta/noise_level = 1/0.5

design = factor(c(rep("0", samples), rep("X", samples), rep("Y", samples), rep("Y+X", samples)))

simulated_values = simulate_from_means(profile_means[1,], samples,
                                       noise_level, exp_min, exp_max)
names(simulated_values) = design

boxplot(simulated_values ~ design, ylab = 'simulated expression', col = 'gray')

stripchart(simulated_values ~ design, vertical = TRUE, 
    method = "jitter", add = TRUE, pch = 20, col = 'black',cex=1.5)

```

Example with more noise.
```{r results = 'hide', message = FALSE, warning = FALSE}
source("setPowerPointStyle.R")
setPowerPointStyle()

noise_level = 1 #this means that the signal-to-noise is delta/noise_level = 1/1

simulated_values = simulate_from_means(profile_means[1,], samples,
                                       noise_level, exp_min, exp_max)

boxplot(simulated_values ~ design, ylab = 'simulated expression', 
        col = 'gray')

stripchart(simulated_values ~ design, vertical = TRUE, 
    method = "jitter", add = TRUE, pch = 20, col = 'black',cex=1.5)

```


#### Extracting statistical features from a noisy input
Let's assume we have a noisy input in the form of N replicates of 0,X, Y, X+Y. We derive a set of statistical features, which will be used as predictors of the true profile in the  classifier. Such features consist of: the Bliss index, the mean expression values in 0,X, Y, X+Y, and the p-values for all possible pairwise tests. We consider both one-tailed, and two-tailed tests of t-test and Wilkoxon. In total, the are 75 variables. These statistical features are computed with the function *match11* as shown below. 

```{r warning = FALSE}
source("extract_stat_features.R")

profile_features = extract_stat_features(simulated_values, design)
length(profile_features)
head(profile_features, 10)
```


### Analyzing a dataset

```{r warning = FALSE}
#data_file = "GSE75003.txt"
data_file = "tnf_ifn_1_v3"
my_data = read.csv(data_file)
#my_data = read.csv(data_file,sep = '\t')
head(my_data)

```

```{r warning=FALSE}
#graphical parameters
source("setPowerPointStyle.R")
setPowerPointStyle()

expression_data = my_data[,-(1:2)]
expression_data = expression_data[-which(apply(expression_data, 1, sd) == 0), ]

if (max(expression_data)>25) expression_data = log2(expression_data)

my.pca <- prcomp(t(expression_data), center = TRUE, scale = TRUE)

#we assume the same number of samples for each condition
samples = ncol(expression_data)/4

cols = c(rep("black", samples), rep("red", samples),
         rep("blue", samples), rep("yellow", samples))

plot(my.pca$x[, 1], my.pca$x[, 2], col = cols,
     xlab = "PC1", ylab = "PC2", pch = 20, cex = 1.5, main = data_file)

legend("bottomleft", pch = 20, col = unique(cols), 
       legend = c("0","X","Y","X+Y"), bty = 'n',cex = 1)
```


### Filtering based on minimum group differences
```{r warning=FALSE}
source("filter_data_on_deltas.R")
design = factor(c(rep("0",samples),rep("X",samples),
                  rep("Y",samples),rep("Y+X",samples)))


my_data_filtered = filter_data_on_deltas(my_data, design = design)
```


### Applying the classification algorithm
```{r warning = FALSE, message = FALSE}
source("find_optimal_match.R")

my_data_filtered_matched = find_optimal_match(my_data_filtered, design, PROFCODES)

head(my_data_filtered_matched)
#save(my_data_filtered_matched, "my_data_filtered_matched_1h")
```


